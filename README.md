# TP 5 - Deep Learning: Sequence Modeling & Attention

## Structure

- **Part 1**: Custom Attention Layer implementation (Exercise 1).
- **Part 2**: Seq2Seq with Cross-Attention & MLflow (Exercise 2).
- **Part 3**: Research Challenge (Placeholder).
- **Rapport**: LaTeX Report.

## Installation

```bash
pip install -r requirements.txt
```

## Running Experiments

### Part 1

```bash
python Part1/experiments/run_experiment.py
```

### Part 2

```bash
python Part2/experiments/train_mlflow.py
```
